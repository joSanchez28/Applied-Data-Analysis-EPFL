{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADA 2018 - Homework 3\n",
    "\n",
    "\n",
    "\n",
    "## Undestanding the StackOverflow community\n",
    "\n",
    "\n",
    "__Deadline: Nov 7th 2018, 23:59:59__\n",
    "\n",
    "__Submission link: Check channel homework-3-public__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__StackOverflow is the most popular programming-related Q&A website. It serves as a platform for users to ask and answer questions and to vote questions and answers up or down. Users of StackOverflow can earn reputation points and \"badges\"; for example, a person is awarded 10 reputation points for receiving an \"up\" vote on an answer given to a question, and 5 points for the \"up\" vote on a question asked. Also, users receive badges for their valued contributions, which represents a kind of gamification of the traditional Q&A site.__\n",
    "\n",
    "[Learn more about StackOverflow on Wikipedia](https://en.wikipedia.org/wiki/Stack_Overflow)\n",
    "\n",
    "----\n",
    "\n",
    "__Dataset link:__\n",
    "\n",
    "https://drive.google.com/open?id=1POlGjqzw9v_pZ_bUnXGihOgk45kbvNjB\n",
    "\n",
    "http://iccluster053.iccluster.epfl.ch/Posts.json.zip (mirror 1)\n",
    "\n",
    "https://iloveadatas.com/datasets/Posts.json.zip (mirror 2)\n",
    "\n",
    "__Dataset description:__\n",
    "\n",
    "* **Id**: Id of the post\n",
    "* **CreationDate**: Creation date of the post (String format)\n",
    "* **PostTypeId**: Type of post (Question = 1, Answer = 2)\n",
    "* **ParentId**: The id of the question. Only present if PostTypeId = 2\n",
    "* **Score**: Points assigned by the users\n",
    "* **Tags**: Tags of the question. Only present if PostTypeId = 1\n",
    "* **Title**: Only present if PostTypeId = 1\n",
    "* **ViewCount**: Only present if PostTypeId = 1\n",
    "\n",
    "__The dataset format is JSON. Here are examples of a question and an answer:__\n",
    "\n",
    "__Question:__\n",
    "```json\n",
    "{\n",
    "    \"Id\": 10130734,\n",
    "    \"CreationDate\": \"2012-04-12T19:51:25.793+02:00\",\n",
    "    \"PostTypeId\": 1,\n",
    "    \"Score\": 4,\n",
    "    \"Tags\": \"<python><pandas>\",\n",
    "    \"Title\": \"Best way to insert a new value\",\n",
    "    \"ViewCount\": 3803\n",
    "}\n",
    "```\n",
    "\n",
    "__Answer:__\n",
    "```json\n",
    "{  \n",
    "   \"CreationDate\":\"2010-10-26T03:19:05.063+02:00\",\n",
    "   \"Id\":4020440,\n",
    "   \"ParentId\":4020214,\n",
    "   \"PostTypeId\":2,\n",
    "   \"Score\":1\n",
    "}\n",
    "```\n",
    "\n",
    "----\n",
    "__Useful resources:__\n",
    "\n",
    "**Spark SQL, DataFrames and Datasets Guide**\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "**Database schema documentation for the public data dump**\n",
    "\n",
    "https://meta.stackexchange.com/questions/2677/database-schema-documentation-for-the-public-data-dump-and-sede\n",
    "\n",
    "**Note:** Use Spark where possible. Some computations can take more than 10 minutes on a common notebook. Consider to save partial results on disk.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "import calendar\n",
    "%matplotlib inline\n",
    "\n",
    "import findspark\n",
    "#findspark.init(r'C:\\Users\\jorge\\Anaconda3\\pkgs\\pyspark-2.3.1-py36_1001\\Lib\\site-packages\\pyspark')\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import min\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task A: Convert the dataset to a more convenient format\n",
    "__As a warm-up task (and to avoid to warm up your laptop too much), load the dataset into a Spark dataframe, show the content, and save it in the _Parquet_ format. Use this step to convert the fields to a more convenient form.__\n",
    "\n",
    "__Answer the following questions:__\n",
    "\n",
    "1. __How many questions have been asked on StackOverflow?__\n",
    "2. __How many answers have been given?__\n",
    "3. __What is the percentage of questions with a score of 0?__\n",
    "\n",
    "__Hint:__ The next tasks involve a time difference. Consider storing time in numeric format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading phase\n",
    "\n",
    "First load the data from the JSON file into a Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = spark.read.load(DATA_DIR+'Posts.json', format=\"json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and reload the dataframe using the parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.write.mode('overwrite').parquet(DATA_DIR + 'posts-parquet')\n",
    "posts = spark.read.parquet(DATA_DIR + 'posts-parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40839832 entries (Q + A)\n",
      "+--------------------+--------+--------+----------+-----+--------------------+--------------------+---------+\n",
      "|        CreationDate|      Id|ParentId|PostTypeId|Score|                Tags|               Title|ViewCount|\n",
      "+--------------------+--------+--------+----------+-----+--------------------+--------------------+---------+\n",
      "|2017-08-17T16:20:...|45740344|45740224|         2|    0|                null|                null|     null|\n",
      "|2017-08-17T16:20:...|45740346|45739185|         2|    1|                null|                null|     null|\n",
      "|2017-08-17T16:20:...|45740348|    null|         1|    2|<flash><react-nat...|Is it possible to...|      143|\n",
      "+--------------------+--------+--------+----------+-----+--------------------+--------------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('There are ' + str(posts.count()) + ' entries (Q + A)')\n",
    "posts.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```show``` command outputs a nice grid outline, while the ```take``` command gives us the raw rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CreationDate='2017-08-17T16:20:17.180+02:00', Id=45740344, ParentId=45740224, PostTypeId=2, Score=0, Tags=None, Title=None, ViewCount=None),\n",
       " Row(CreationDate='2017-08-17T16:20:25.720+02:00', Id=45740346, ParentId=45739185, PostTypeId=2, Score=1, Tags=None, Title=None, ViewCount=None),\n",
       " Row(CreationDate='2017-08-17T16:20:28.873+02:00', Id=45740348, ParentId=None, PostTypeId=1, Score=2, Tags='<flash><react-native>', Title='Is it possible to embed Adobe flash into a react-native app?', ViewCount=143)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, register the dataframe as a table to enable SQL querying. This will make part A very easy and manageable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.registerTempTable('posts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qeurying phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. How many questions have been asked on StackOverflow?__\n",
    "\n",
    "Recall: **PostTypeId**: Type of post (Question = 1, Answer = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|total_questions|\n",
      "+---------------+\n",
      "|       15647060|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query1 = \"\"\"\n",
    "select count(*)\n",
    "from posts\n",
    "where PostTypeId = 1\n",
    "\"\"\"\n",
    "result1 = spark.sql(query1)\n",
    "result1 = result1.select(col(\"count(1)\").alias(\"total_questions\"))\n",
    "result1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. How many answers have been given?__\n",
    "\n",
    "Recall: **PostTypeId**: Type of post (Question = 1, Answer = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|total_answers|\n",
      "+-------------+\n",
      "|     25192772|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query2 = \"\"\"\n",
    "select count(*)\n",
    "from posts\n",
    "where PostTypeId = 2\n",
    "\"\"\"\n",
    "result2 = spark.sql(query2)\n",
    "result2 = result2.select(col(\"count(1)\").alias(\"total_answers\"))\n",
    "result2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. What is the percentage of questions with a score of 0?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|questions_scored_0|\n",
      "+------------------+\n",
      "|           9833069|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query31=\"\"\"\n",
    "select count(*)\n",
    "from posts\n",
    "where PostTypeId = 2 and Score = 0\n",
    "\"\"\"\n",
    "result31 = spark.sql(query31)\n",
    "result31 = result31.select(col(\"count(1)\").alias(\"questions_scored_0\"))\n",
    "result31.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.842917455419744"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_scored0 = result31.collect()[0].questions_scored_0\n",
    "total_questions = result1.collect()[0].total_questions \n",
    "percentage_scored0 = q_scored0/total_questions * 100\n",
    "percentage_scored0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint:** Load the dataset from the Parquet file for the next tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B: What are the 10 most popular tags?\n",
    "\n",
    "__What are the most popular tags in StackOverflow? Use Spark to extract the information you need, and answer the following questions with Pandas and Matplotlib (or Seaborn):__\n",
    "\n",
    "1. __What is the proportion of tags that appear in fewer than 100 questions?__\n",
    "2. __Plot the distribution of the tag counts using an appropriate representation.__\n",
    "3. __Plot a bar chart with the number of questions for the 10 most popular tags.__\n",
    "\n",
    "__For each task describe your findings briefly.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. What is the proportion of tags that appear in fewer than 100 questions?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will use SQL to speed up our querying\n",
    "\n",
    "First, we separate the string in the \"Tags\" column that includes all of the tags into an array with the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------+----------+-----+----+-----+---------+\n",
      "|        CreationDate|      Id|ParentId|PostTypeId|Score|Tags|Title|ViewCount|\n",
      "+--------------------+--------+--------+----------+-----+----+-----+---------+\n",
      "|2017-08-17T16:20:...|45740344|45740224|         2|    0|null| null|     null|\n",
      "|2017-08-17T16:20:...|45740346|45739185|         2|    1|null| null|     null|\n",
      "+--------------------+--------+--------+----------+-----+----+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tags column, from string to array\n",
    "postsb = posts.withColumn(\"Tags\", split(posts[\"Tags\"],\"><\").cast(\"array<String>\"))\n",
    "postsb.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can extend the table using explode in these arrays, creating rows for every array position (for every tag of each post)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------+----------+-----+------------+--------------------+---------+\n",
      "|        CreationDate|      Id|ParentId|PostTypeId|Score|        Tags|               Title|ViewCount|\n",
      "+--------------------+--------+--------+----------+-----+------------+--------------------+---------+\n",
      "|2017-08-17T16:20:...|45740348|    null|         1|    2|       flash|Is it possible to...|      143|\n",
      "|2017-08-17T16:20:...|45740348|    null|         1|    2|react-native|Is it possible to...|      143|\n",
      "+--------------------+--------+--------+----------+-----+------------+--------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "postsb = postsb.withColumn(\"Tags\",  explode(\"Tags\")) #Explode the table in order to have one row for every tag in each post\n",
    "charReplace = udf(lambda x: x.replace(\"<\" ,'').replace(\">\", '')) #For deleting \"<\" and \">\" in the tag names\n",
    "postsb = postsb.withColumn('Tags',charReplace('Tags'))\n",
    "postsb.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we just have to count how many rows there are grouping by category (we count the total number and the tags which appear less than 100 times). We also have to consider that we are asked about the proportion of questions (not answers, PostTypeId=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52994"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryb11=\"\"\"\n",
    "select Tags, count(*) as appearances\n",
    "from postsb\n",
    "where PostTypeId = 1\n",
    "group by Tags\n",
    "\"\"\"\n",
    "resultb11 = spark.sql(queryb11)\n",
    "#resultb11.show(3)\n",
    "total_tags = resultb11.count()\n",
    "total_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36025"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of tags which appear less than 100 times\n",
    "resultb12 = resultb11.filter('appearances < 100')   #We take advantage of the last query\n",
    "#resultb12.show(3)\n",
    "tags_less_100=resultb12.count()\n",
    "tags_less_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6797939389364833"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proportion = tags_less_100/total_tags\n",
    "proportion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Plot the distribution of the tag counts using an appropriate representation.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a pandas dataframe here for comfort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tags</th>\n",
       "      <th>appearances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>response.write</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iframe</td>\n",
       "      <td>26125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>input</td>\n",
       "      <td>22045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arguments</td>\n",
       "      <td>7157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Tags  appearances\n",
       "0  response.write          137\n",
       "1          iframe        26125\n",
       "2           input        22045\n",
       "3       arguments         7157"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_counts_pd = resultb11.toPandas()\n",
    "tag_counts_pd.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tag_counts_pd.hist(figsize=(15,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are some outliers. Most of the tags have less than 200000 questions (which is still a lot). In the next question we look a those very popular outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Plot a bar chart with the number of questions for the 10 most popular tags.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we compute the data frame with the 10 most popular tags (and their appearances in questions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>appearances</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>javascript</th>\n",
       "      <td>1585495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>java</th>\n",
       "      <td>1352735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c#</th>\n",
       "      <td>1172492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>php</th>\n",
       "      <td>1136082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>android</th>\n",
       "      <td>1088811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>python</th>\n",
       "      <td>954401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jquery</th>\n",
       "      <td>886982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>html</th>\n",
       "      <td>725289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ios</th>\n",
       "      <td>551843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c++</th>\n",
       "      <td>536267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            appearances\n",
       "Tags                   \n",
       "javascript      1585495\n",
       "java            1352735\n",
       "c#              1172492\n",
       "php             1136082\n",
       "android         1088811\n",
       "python           954401\n",
       "jquery           886982\n",
       "html             725289\n",
       "ios              551843\n",
       "c++              536267"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp=tag_counts_pd.set_index(\"Tags\").sort_values(by=\"appearances\", ascending=False).head(10)\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data frame, plotting the bar chart is trivial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1e872caec50>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_tmp.plot.bar(figsize=(15,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task C: View-score relation\n",
    "\n",
    "__We want to investigate the correlation between the view count and the score of questions.__\n",
    "\n",
    "1. __Get the view count and score of the questions with tag ```random-effects``` and visualize the relation between these two variables using an appropriate plot.__\n",
    "2. __Are these two variables correlated? Use the Pearson coefficient to validate your hypothesis. Discuss your findings in detail.__\n",
    "\n",
    "__Hint:__ Inspect the data visually before drawing your conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Get the view count and score of the questions with tag random-effects and visualize the relation between these two variables using an appropriate plot.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we obtain a table with the view counts and score of the questions with tag *random-effects*. We have done it by SQL and by the table \"postsb\" obtained in task B.\n",
    "\n",
    "(In this table we already have the tag vectors exploded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|ViewCount|Score|\n",
      "+---------+-----+\n",
      "|     5345|   -2|\n",
      "|      852|    4|\n",
      "|    42678|   25|\n",
      "|      275|    0|\n",
      "+---------+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queryc1 = \"\"\"\n",
    "select ViewCount, Score\n",
    "from postsb\n",
    "where Tags = \"random-effects\" and PostTypeId=1\n",
    "\"\"\"\n",
    "resultc1 = spark.sql(queryc1)\n",
    "resultc1.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this table is easy to get a visualization between both variables (considering just in questions with the asked tag). Note that we have done it by transforming the table to a pandas data frame and using the seaborn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\socket.py:647: ResourceWarning: unclosed <socket.socket fd=2508, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 65053), raddr=('127.0.0.1', 65052)>\n",
      "  self._sock = None\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lmplot(x='ViewCount', y='Score', data=resultc1.toPandas());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Are these two variables correlated? Use the Pearson coefficient to validate your hypothesis. Discuss your findings in detail.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the Pearson correlation coefficient directly with the last exercise result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8821972419941506"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultc1.corr(\"ViewCount\", \"Score\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pearson Correlation Coefficient (PCC) is a measure of the linear correlation between two variables X and Y. It has a value between +1 and 1, where 1 is total positive linear correlation.\n",
    "\n",
    "In this way, it seems to make sense to obtain PCC=0.8821... because this mean that there exists a great correlation between both variables(*ViewCount* and *Score*). As you can see in the last visualization of the asked data set (*ViewCount* and *Score* of every question with *random-effects* tag) one variable usually decreases as the other variable decreases and one variable usually increases while the other increases. \n",
    "\n",
    "(Reminder: In statistics, a perfect positive correlation is represented by 1, while 0 indicates no correlation, and negative 1 indicates a perfect negative correlation.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task D: What are the tags with the fastest first answer?\n",
    "\n",
    "__What are the tags that have the fastest response time from the community? We define the response time as the difference in seconds between the timestamps of the question and of the first answer received.__\n",
    "\n",
    "1. __Get the response time for the first answer of the questions with the tags ```python``` and ```java```.__\n",
    "2. __Plot the two distributions in an appropriate format. What do you observe? Describe your findings and discuss the following distribution properties: mean, median, standard deviation.__\n",
    "3. __We believe that the response time is lower for questions related to Python (compare to Java). Contradict or confirm this assumption by estimating the proper statistic with bootstrapping. Visualize the 95% confidence intervals with box plots and describe your findings.__\n",
    "4. __Repeat the first analysis (D1) by using the proper statistic to measure the response time for the tags that appear at least 5000 times. Plot the distribution of the 10 tags with the fastest response time.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Loading phase\n",
    "\n",
    "First we create the dataframes we are going to use, save them in parquet format and reload them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_questions = postsb.filter('Tags = \"java\"')\n",
    "java_answers = posts.filter('PostTypeId = 2').select('CreationDate',col('ParentId').alias('Id'),'PostTypeId')\n",
    "\n",
    "java_questions.write.mode('overwrite').parquet(DATA_DIR + \"java_questions.parquet\")\n",
    "java_answers.write.mode('overwrite').parquet(DATA_DIR + \"java_answers.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_questions = postsb.filter('Tags = \"python\"')\n",
    "python_answers = posts.filter('PostTypeId = 2')\n",
    "\n",
    "python_questions.write.mode('overwrite').parquet(DATA_DIR + \"python_questions.parquet\")\n",
    "python_answers.write.mode('overwrite').parquet(DATA_DIR + \"python_answers.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_questions = spark.read.parquet(DATA_DIR + \"java_questions.parquet\")\n",
    "java_answers = spark.read.parquet(DATA_DIR + \"java_answers.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_questions = spark.read.parquet(DATA_DIR + \"python_questions.parquet\")\n",
    "python_answers = spark.read.parquet(DATA_DIR + \"python_answers.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Get the response time for the first answer of the questions with the tags ```python``` and ```java```.__\n",
    "\n",
    "In order to get the difference in seconds we first need to transform the date strings into timestamps. For this we will use datetime objects, from which it is quite easy to go to timestamp.\n",
    "\n",
    "First we format the string so it can be properly read and transformed into a datetime object.\n",
    "\n",
    "Note that in order to join the two dataframes (questions and answers), we need them to have a common column, which is going to be the 'Id' for the questions and 'ParentId' for the answers, which we rename to 'Id', forgetting the actual 'Id' of the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateFormat = udf(lambda x: x[:23] + \"000\" + x[23:26] + x[27:])\n",
    "java_answers = posts.filter('PostTypeId = 2') \\\n",
    "                    .select(col('CreationDate').alias('AnswerCreationDate'),col('ParentId').alias('Id')) \\\n",
    "                    .withColumn('AnswerCreationDate',dateFormat('AnswerCreationDate'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now join questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_q_and_a = java_questions.join(java_answers,'Id') \\\n",
    "                             .select('Id', col('CreationDate').alias('QuestionCreationDate'),'AnswerCreationDate') \\\n",
    "                             .withColumn('QuestionCreationDate', dateFormat('QuestionCreationDate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\socket.py:647: ResourceWarning: unclosed <socket.socket fd=2356, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 64903), raddr=('127.0.0.1', 64902)>\n",
      "  self._sock = None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>QuestionCreationDate</th>\n",
       "      <th>AnswerCreationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30428</td>\n",
       "      <td>2008-08-27T15:21:31.830000+0200</td>\n",
       "      <td>2008-12-21T00:07:13.363000+0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30428</td>\n",
       "      <td>2008-08-27T15:21:31.830000+0200</td>\n",
       "      <td>2008-10-29T17:56:04.147000+0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30428</td>\n",
       "      <td>2008-08-27T15:21:31.830000+0200</td>\n",
       "      <td>2008-11-11T03:00:57.220000+0100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id             QuestionCreationDate               AnswerCreationDate\n",
       "0  30428  2008-08-27T15:21:31.830000+0200  2008-12-21T00:07:13.363000+0100\n",
       "1  30428  2008-08-27T15:21:31.830000+0200  2008-10-29T17:56:04.147000+0100\n",
       "2  30428  2008-08-27T15:21:31.830000+0200  2008-11-11T03:00:57.220000+0100"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "java_q_and_a_df = java_q_and_a.toPandas()\n",
    "java_q_and_a_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the string with the date into a datetime object and then into its timestamp value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>QuestionCreationDate</th>\n",
       "      <th>AnswerCreationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30428</td>\n",
       "      <td>2008-08-27T15:21:31.830000+0200</td>\n",
       "      <td>1229818033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30428</td>\n",
       "      <td>2008-08-27T15:21:31.830000+0200</td>\n",
       "      <td>1225302964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30428</td>\n",
       "      <td>2008-08-27T15:21:31.830000+0200</td>\n",
       "      <td>1226372457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id             QuestionCreationDate  AnswerCreationDate\n",
       "0  30428  2008-08-27T15:21:31.830000+0200          1229818033\n",
       "1  30428  2008-08-27T15:21:31.830000+0200          1225302964\n",
       "2  30428  2008-08-27T15:21:31.830000+0200          1226372457"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "java_q_and_a_df['AnswerCreationDate'] = java_q_and_a_df['AnswerCreationDate'].apply(lambda x: datetime.strptime(x,\"%Y-%m-%dT%H:%M:%S.%f%z\"))\n",
    "java_q_and_a_df['AnswerCreationDate'] = java_q_and_a_df['AnswerCreationDate'].apply(lambda x: calendar.timegm(x.timetuple()))\n",
    "java_q_and_a_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>QuestionCreationDate</th>\n",
       "      <th>AnswerCreationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30428</td>\n",
       "      <td>1219850491</td>\n",
       "      <td>1229818033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30428</td>\n",
       "      <td>1219850491</td>\n",
       "      <td>1225302964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30428</td>\n",
       "      <td>1219850491</td>\n",
       "      <td>1226372457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  QuestionCreationDate  AnswerCreationDate\n",
       "0  30428            1219850491          1229818033\n",
       "1  30428            1219850491          1225302964\n",
       "2  30428            1219850491          1226372457"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "java_q_and_a_df['QuestionCreationDate'] = java_q_and_a_df['QuestionCreationDate'].apply(lambda x: datetime.strptime(x,\"%Y-%m-%dT%H:%M:%S.%f%z\"))\n",
    "java_q_and_a_df['QuestionCreationDate'] = java_q_and_a_df['QuestionCreationDate'].apply(lambda x: calendar.timegm(x.timetuple()))\n",
    "java_q_and_a_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we group all the answers with the same question and sort by answer timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>QuestionCreationDate</th>\n",
       "      <th>AnswerCreationDate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th>QuestionCreationDate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <th>1217606932</th>\n",
       "      <td>123</td>\n",
       "      <td>1217606932</td>\n",
       "      <td>1217608303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <th>1217667507</th>\n",
       "      <td>382</td>\n",
       "      <td>1217667507</td>\n",
       "      <td>1217667528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <th>1217713654</th>\n",
       "      <td>564</td>\n",
       "      <td>1217713654</td>\n",
       "      <td>1217714145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Id  QuestionCreationDate  AnswerCreationDate\n",
       "Id  QuestionCreationDate                                               \n",
       "123 1217606932            123            1217606932          1217608303\n",
       "382 1217667507            382            1217667507          1217667528\n",
       "564 1217713654            564            1217713654          1217714145"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "java_q_and_a_df = java_q_and_a_df.groupby(['Id', 'QuestionCreationDate']).apply(lambda x: x.sort_values(by='AnswerCreationDate', ascending=True).iloc[0])\n",
    "java_q_and_a_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we repeat the whole process for the python tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\socket.py:647: ResourceWarning: unclosed <socket.socket fd=1124, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=0, laddr=('127.0.0.1', 65491), raddr=('127.0.0.1', 65490)>\n",
      "  self._sock = None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>QuestionCreationDate</th>\n",
       "      <th>AnswerCreationDate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th>QuestionCreationDate</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <th>1217689876</th>\n",
       "      <td>469</td>\n",
       "      <td>1217689876</td>\n",
       "      <td>1217696213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <th>1217696518</th>\n",
       "      <td>502</td>\n",
       "      <td>1217696518</td>\n",
       "      <td>1217702947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <th>1217702634</th>\n",
       "      <td>535</td>\n",
       "      <td>1217702634</td>\n",
       "      <td>1217703416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Id  QuestionCreationDate  AnswerCreationDate\n",
       "Id  QuestionCreationDate                                               \n",
       "469 1217689876            469            1217689876          1217696213\n",
       "502 1217696518            502            1217696518          1217702947\n",
       "535 1217702634            535            1217702634          1217703416"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter the columns we want, format the date string\n",
    "python_answers = posts.filter('PostTypeId = 2') \\\n",
    "                    .select(col('CreationDate') \\\n",
    "                    .alias('AnswerCreationDate'),col('ParentId').alias('Id')) \\\n",
    "                    .withColumn('AnswerCreationDate',dateFormat('AnswerCreationDate'))\n",
    "\n",
    "# join Q&A\n",
    "python_q_and_a = python_questions.join(python_answers,'Id') \\\n",
    "                             .select('Id', col('CreationDate').alias('QuestionCreationDate'),'AnswerCreationDate') \\\n",
    "                             .withColumn('QuestionCreationDate', dateFormat('QuestionCreationDate'))\n",
    "\n",
    "# transform into pd dataframe\n",
    "python_q_and_a_df = python_q_and_a.toPandas()\n",
    "\n",
    "# get timestamp for answers\n",
    "python_q_and_a_df['AnswerCreationDate'] = python_q_and_a_df['AnswerCreationDate'].apply(lambda x: datetime.strptime(x,\"%Y-%m-%dT%H:%M:%S.%f%z\"))\n",
    "python_q_and_a_df['AnswerCreationDate'] = python_q_and_a_df['AnswerCreationDate'].apply(lambda x: calendar.timegm(x.timetuple()))\n",
    "\n",
    "# get timestamp for answers\n",
    "python_q_and_a_df['QuestionCreationDate'] = python_q_and_a_df['QuestionCreationDate'].apply(lambda x: datetime.strptime(x,\"%Y-%m-%dT%H:%M:%S.%f%z\"))\n",
    "python_q_and_a_df['QuestionCreationDate'] = python_q_and_a_df['QuestionCreationDate'].apply(lambda x: calendar.timegm(x.timetuple()))\n",
    "\n",
    "# group by question and sort by timestamp\n",
    "python_q_and_a_df = python_q_and_a_df.groupby(['Id', 'QuestionCreationDate']).apply(lambda x: x.sort_values(by='AnswerCreationDate', ascending=True).iloc[0])\n",
    "python_q_and_a_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Plot the two distributions in an appropriate format. What do you observe? Describe your findings and discuss the following distribution properties: mean, median, standard deviation.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a column for the response time (difference between the question timestamp and the answer timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_q_and_a_df['response time'] = java_q_and_a_df['QuestionCreationDate'] - java_q_and_a_df['AnswerCreationDate']\n",
    "java_plot_df = java_q_and_a_df[['Id', 'response time']].sort_values(by='response time', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_q_and_a_df['response time'] = java_q_and_a_df['QuestionCreationDate'] - java_q_and_a_df['AnswerCreationDate']\n",
    "python_q_and_a_df['response time'] = python_q_and_a_df['QuestionCreationDate'] - python_q_and_a_df['AnswerCreationDate']\n",
    "\n",
    "java_plot_df = java_q_and_a_df[['Id', 'response time']].sort_values(by='response time', ascending = True)\n",
    "python_plot_df = python_q_and_a_df[['Id', 'response time']].sort_values(by='response time', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-111-a7154a86e083>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mjava_plot_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bar'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Question ID'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Response time (seconds)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Response Time for Questions with the Java tag'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_core.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)\u001b[0m\n\u001b[0;32m   2939\u001b[0m                           \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolormap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2940\u001b[0m                           \u001b[0myerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecondary_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2941\u001b[1;33m                           sort_columns=sort_columns, **kwds)\n\u001b[0m\u001b[0;32m   2942\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplot_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_core.py\u001b[0m in \u001b[0;36mplot_frame\u001b[1;34m(data, x, y, kind, ax, subplots, sharex, sharey, layout, figsize, use_index, title, grid, legend, style, logx, logy, loglog, xticks, yticks, xlim, ylim, rot, fontsize, colormap, table, yerr, xerr, secondary_y, sort_columns, **kwds)\u001b[0m\n\u001b[0;32m   1975\u001b[0m                  \u001b[0myerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myerr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxerr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxerr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1976\u001b[0m                  \u001b[0msecondary_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msecondary_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msort_columns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1977\u001b[1;33m                  **kwds)\n\u001b[0m\u001b[0;32m   1978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_core.py\u001b[0m in \u001b[0;36m_plot\u001b[1;34m(data, x, y, subplots, ax, kind, **kwds)\u001b[0m\n\u001b[0;32m   1802\u001b[0m         \u001b[0mplot_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubplots\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1803\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1804\u001b[1;33m     \u001b[0mplot_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1805\u001b[0m     \u001b[0mplot_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1806\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mplot_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_core.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_plot_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup_subplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_legend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_core.py\u001b[0m in \u001b[0;36m_make_plot\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1263\u001b[0m                 rect = self._plot(ax, self.ax_pos + (i + 0.5) * w, y, w,\n\u001b[0;32m   1264\u001b[0m                                   \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1265\u001b[1;33m                                   log=self.log, **kwds)\n\u001b[0m\u001b[0;32m   1266\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_legend_handle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\plotting\\_core.py\u001b[0m in \u001b[0;36m_plot\u001b[1;34m(cls, ax, x, y, w, start, log, **kwds)\u001b[0m\n\u001b[0;32m   1207\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1209\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1211\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1853\u001b[0m                         \u001b[1;34m\"the Matplotlib list!)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1855\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1856\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1857\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mbar\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2284\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0morientation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'horizontal'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2285\u001b[0m                 \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msticky_edges\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2286\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_patch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2287\u001b[0m             \u001b[0mpatches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36madd_patch\u001b[1;34m(self, p)\u001b[0m\n\u001b[0;32m   2004\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_artist_props\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2005\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2006\u001b[1;33m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2007\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_patch_limits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mset_clip_path\u001b[1;34m(self, path, transform)\u001b[0m\n\u001b[0;32m    671\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRectangle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m                 self.clipbox = TransformedBbox(Bbox.unit(),\n\u001b[1;32m--> 673\u001b[1;33m                                                path.get_transform())\n\u001b[0m\u001b[0;32m    674\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clippath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m                 \u001b[0msuccess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\patches.py\u001b[0m in \u001b[0;36mget_transform\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPatch\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \"\"\"\n\u001b[1;32m--> 199\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_patch_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mArtist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_data_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\patches.py\u001b[0m in \u001b[0;36mget_patch_transform\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_patch_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_patch_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    738\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rect_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\patches.py\u001b[0m in \u001b[0;36m_update_patch_transform\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[0mbbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_extents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m         \u001b[0mrot_trans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAffine2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m         \u001b[0mrot_trans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrotate_deg_around\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mangle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rect_transform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBboxTransformTo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rect_transform\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mrot_trans\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\transforms.py\u001b[0m in \u001b[0;36mrotate_deg_around\u001b[1;34m(self, x, y, degrees)\u001b[0m\n\u001b[0;32m   2058\u001b[0m         \u001b[1;31m# Cast to float to avoid wraparound issues with uint8's\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2059\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2060\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrotate_deg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2061\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2062\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mty\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\transforms.py\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(self, tx, ty)\u001b[0m\n\u001b[0;32m   2070\u001b[0m         translate_mtx = np.array(\n\u001b[0;32m   2071\u001b[0m             [[1.0, 0.0, tx], [0.0, 1.0, ty], [0.0, 0.0, 1.0]], float)\n\u001b[1;32m-> 2072\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mtx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranslate_mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2073\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2074\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 61277)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "sys:1: ResourceWarning: unclosed file <_io.BufferedWriter name=4>\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:61209)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1145, in send_command\n",
      "    self.socket.sendall(command.encode(\"utf-8\"))\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1149, in send_command\n",
      "    \"Error while sending\", e, proto.ERROR_ON_SEND)\n",
      "py4j.protocol.Py4JNetworkError: Error while sending\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1145, in send_command\n",
      "    self.socket.sendall(command.encode(\"utf-8\"))\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1149, in send_command\n",
      "    \"Error while sending\", e, proto.ERROR_ON_SEND)\n",
      "py4j.protocol.Py4JNetworkError: Error while sending\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\logging\\__init__.py\", line 988, in emit\n",
      "    stream.write(msg)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 408, in write\n",
      "    self._schedule_flush()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 332, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 205, in schedule\n",
      "    f()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 331, in _schedule_in_thread\n",
      "    self._io_loop.call_later(self.flush_interval, self._flush)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 638, in call_later\n",
      "    return self.call_at(self.time() + delay, callback, *args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 145, in call_at\n",
      "    functools.partial(stack_context.wrap(callback), *args, **kwargs))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 542, in call_later\n",
      "    timer = self.call_at(self.time() + delay, callback, *args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 552, in call_at\n",
      "    self._check_closed()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 357, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n",
      "Call stack:\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1293, in <lambda>\n",
      "    _garbage_collect_object and _garbage_collect_object(cc, id))\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 625, in _garbage_collect_object\n",
      "    gateway_client.garbage_collect_object(target_id)\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 920, in garbage_collect_object\n",
      "    \"\\ne\\n\")\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1000, in send_command\n",
      "    response = self.send_command(command, binary=binary)\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 983, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 931, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 937, in _create_connection\n",
      "    connection.start()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1078, in start\n",
      "    logger.exception(msg)\n",
      "Message: 'An error occurred while trying to connect to the Java server (127.0.0.1:61209)'\n",
      "Arguments: ()\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:61209)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:61209)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\logging\\__init__.py\", line 988, in emit\n",
      "    stream.write(msg)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 408, in write\n",
      "    self._schedule_flush()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 332, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 205, in schedule\n",
      "    f()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 331, in _schedule_in_thread\n",
      "    self._io_loop.call_later(self.flush_interval, self._flush)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 638, in call_later\n",
      "    return self.call_at(self.time() + delay, callback, *args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 145, in call_at\n",
      "    functools.partial(stack_context.wrap(callback), *args, **kwargs))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 542, in call_later\n",
      "    timer = self.call_at(self.time() + delay, callback, *args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 552, in call_at\n",
      "    self._check_closed()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 357, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n",
      "Call stack:\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1293, in <lambda>\n",
      "    _garbage_collect_object and _garbage_collect_object(cc, id))\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 625, in _garbage_collect_object\n",
      "    gateway_client.garbage_collect_object(target_id)\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 920, in garbage_collect_object\n",
      "    \"\\ne\\n\")\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 983, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 931, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 937, in _create_connection\n",
      "    connection.start()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1078, in start\n",
      "    logger.exception(msg)\n",
      "Message: 'An error occurred while trying to connect to the Java server (127.0.0.1:61209)'\n",
      "Arguments: ()\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:61209)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:61209)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\logging\\__init__.py\", line 988, in emit\n",
      "    stream.write(msg)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 408, in write\n",
      "    self._schedule_flush()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 332, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 205, in schedule\n",
      "    f()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 331, in _schedule_in_thread\n",
      "    self._io_loop.call_later(self.flush_interval, self._flush)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 638, in call_later\n",
      "    return self.call_at(self.time() + delay, callback, *args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 145, in call_at\n",
      "    functools.partial(stack_context.wrap(callback), *args, **kwargs))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 542, in call_later\n",
      "    timer = self.call_at(self.time() + delay, callback, *args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 552, in call_at\n",
      "    self._check_closed()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 357, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n",
      "Call stack:\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1293, in <lambda>\n",
      "    _garbage_collect_object and _garbage_collect_object(cc, id))\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 625, in _garbage_collect_object\n",
      "    gateway_client.garbage_collect_object(target_id)\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 920, in garbage_collect_object\n",
      "    \"\\ne\\n\")\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 983, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 931, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 937, in _create_connection\n",
      "    connection.start()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1078, in start\n",
      "    logger.exception(msg)\n",
      "Message: 'An error occurred while trying to connect to the Java server (127.0.0.1:61209)'\n",
      "Arguments: ()\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:61209)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:61209)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\logging\\__init__.py\", line 988, in emit\n",
      "    stream.write(msg)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 408, in write\n",
      "    self._schedule_flush()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 332, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 205, in schedule\n",
      "    f()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 331, in _schedule_in_thread\n",
      "    self._io_loop.call_later(self.flush_interval, self._flush)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 638, in call_later\n",
      "    return self.call_at(self.time() + delay, callback, *args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 145, in call_at\n",
      "    functools.partial(stack_context.wrap(callback), *args, **kwargs))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 542, in call_later\n",
      "    timer = self.call_at(self.time() + delay, callback, *args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 552, in call_at\n",
      "    self._check_closed()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 357, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n",
      "Call stack:\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1293, in <lambda>\n",
      "    _garbage_collect_object and _garbage_collect_object(cc, id))\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 625, in _garbage_collect_object\n",
      "    gateway_client.garbage_collect_object(target_id)\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 920, in garbage_collect_object\n",
      "    \"\\ne\\n\")\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 983, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 931, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 937, in _create_connection\n",
      "    connection.start()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1078, in start\n",
      "    logger.exception(msg)\n",
      "Message: 'An error occurred while trying to connect to the Java server (127.0.0.1:61209)'\n",
      "Arguments: ()\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:61209)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:61209)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\logging\\__init__.py\", line 988, in emit\n",
      "    stream.write(msg)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 408, in write\n",
      "    self._schedule_flush()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 332, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 205, in schedule\n",
      "    f()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 331, in _schedule_in_thread\n",
      "    self._io_loop.call_later(self.flush_interval, self._flush)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 638, in call_later\n",
      "    return self.call_at(self.time() + delay, callback, *args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 145, in call_at\n",
      "    functools.partial(stack_context.wrap(callback), *args, **kwargs))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 542, in call_later\n",
      "    timer = self.call_at(self.time() + delay, callback, *args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 552, in call_at\n",
      "    self._check_closed()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 357, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n",
      "Call stack:\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1293, in <lambda>\n",
      "    _garbage_collect_object and _garbage_collect_object(cc, id))\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 625, in _garbage_collect_object\n",
      "    gateway_client.garbage_collect_object(target_id)\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 920, in garbage_collect_object\n",
      "    \"\\ne\\n\")\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 983, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 931, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 937, in _create_connection\n",
      "    connection.start()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1078, in start\n",
      "    logger.exception(msg)\n",
      "Message: 'An error occurred while trying to connect to the Java server (127.0.0.1:61209)'\n",
      "Arguments: ()\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:61209)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:61209)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\logging\\__init__.py\", line 988, in emit\n",
      "    stream.write(msg)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 408, in write\n",
      "    self._schedule_flush()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 332, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 205, in schedule\n",
      "    f()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 331, in _schedule_in_thread\n",
      "    self._io_loop.call_later(self.flush_interval, self._flush)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 638, in call_later\n",
      "    return self.call_at(self.time() + delay, callback, *args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 145, in call_at\n",
      "    functools.partial(stack_context.wrap(callback), *args, **kwargs))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 542, in call_later\n",
      "    timer = self.call_at(self.time() + delay, callback, *args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 552, in call_at\n",
      "    self._check_closed()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 357, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n",
      "Call stack:\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1293, in <lambda>\n",
      "    _garbage_collect_object and _garbage_collect_object(cc, id))\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 625, in _garbage_collect_object\n",
      "    gateway_client.garbage_collect_object(target_id)\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 920, in garbage_collect_object\n",
      "    \"\\ne\\n\")\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 983, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 931, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 937, in _create_connection\n",
      "    connection.start()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1078, in start\n",
      "    logger.exception(msg)\n",
      "Message: 'An error occurred while trying to connect to the Java server (127.0.0.1:61209)'\n",
      "Arguments: ()\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:61209)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:61209)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\logging\\__init__.py\", line 988, in emit\n",
      "    stream.write(msg)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 408, in write\n",
      "    self._schedule_flush()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 332, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 205, in schedule\n",
      "    f()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 331, in _schedule_in_thread\n",
      "    self._io_loop.call_later(self.flush_interval, self._flush)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 638, in call_later\n",
      "    return self.call_at(self.time() + delay, callback, *args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 145, in call_at\n",
      "    functools.partial(stack_context.wrap(callback), *args, **kwargs))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 542, in call_later\n",
      "    timer = self.call_at(self.time() + delay, callback, *args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 552, in call_at\n",
      "    self._check_closed()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 357, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n",
      "Call stack:\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1293, in <lambda>\n",
      "    _garbage_collect_object and _garbage_collect_object(cc, id))\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 625, in _garbage_collect_object\n",
      "    gateway_client.garbage_collect_object(target_id)\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 920, in garbage_collect_object\n",
      "    \"\\ne\\n\")\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 983, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 931, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 937, in _create_connection\n",
      "    connection.start()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1078, in start\n",
      "    logger.exception(msg)\n",
      "Message: 'An error occurred while trying to connect to the Java server (127.0.0.1:61209)'\n",
      "Arguments: ()\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:61209)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:61209)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\logging\\__init__.py\", line 988, in emit\n",
      "    stream.write(msg)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 408, in write\n",
      "    self._schedule_flush()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 332, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 205, in schedule\n",
      "    f()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 331, in _schedule_in_thread\n",
      "    self._io_loop.call_later(self.flush_interval, self._flush)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 638, in call_later\n",
      "    return self.call_at(self.time() + delay, callback, *args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 145, in call_at\n",
      "    functools.partial(stack_context.wrap(callback), *args, **kwargs))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 542, in call_later\n",
      "    timer = self.call_at(self.time() + delay, callback, *args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 552, in call_at\n",
      "    self._check_closed()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 357, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n",
      "Call stack:\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1293, in <lambda>\n",
      "    _garbage_collect_object and _garbage_collect_object(cc, id))\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 625, in _garbage_collect_object\n",
      "    gateway_client.garbage_collect_object(target_id)\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 920, in garbage_collect_object\n",
      "    \"\\ne\\n\")\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 983, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 931, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 937, in _create_connection\n",
      "    connection.start()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1078, in start\n",
      "    logger.exception(msg)\n",
      "Message: 'An error occurred while trying to connect to the Java server (127.0.0.1:61209)'\n",
      "Arguments: ()\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:61209)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:61209)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\logging\\__init__.py\", line 988, in emit\n",
      "    stream.write(msg)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 408, in write\n",
      "    self._schedule_flush()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 332, in _schedule_flush\n",
      "    self.pub_thread.schedule(_schedule_in_thread)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 205, in schedule\n",
      "    f()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\", line 331, in _schedule_in_thread\n",
      "    self._io_loop.call_later(self.flush_interval, self._flush)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 638, in call_later\n",
      "    return self.call_at(self.time() + delay, callback, *args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 145, in call_at\n",
      "    functools.partial(stack_context.wrap(callback), *args, **kwargs))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 542, in call_later\n",
      "    timer = self.call_at(self.time() + delay, callback, *args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 552, in call_at\n",
      "    self._check_closed()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 357, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n",
      "Call stack:\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1293, in <lambda>\n",
      "    _garbage_collect_object and _garbage_collect_object(cc, id))\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 625, in _garbage_collect_object\n",
      "    gateway_client.garbage_collect_object(target_id)\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 920, in garbage_collect_object\n",
      "    \"\\ne\\n\")\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 983, in send_command\n",
      "    connection = self._get_connection()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 931, in _get_connection\n",
      "    connection = self._create_connection()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 937, in _create_connection\n",
      "    connection.start()\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1078, in start\n",
      "    logger.exception(msg)\n",
      "Message: 'An error occurred while trying to connect to the Java server (127.0.0.1:61209)'\n",
      "Arguments: ()\n",
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:61209)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 929, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Spark\\spark-2.3.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\", line 1067, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [WinError 10061] No connection could be made because the target machine actively refused it\n"
     ]
    }
   ],
   "source": [
    "java_plot_df.plot(kind='bar',use_index=False);\n",
    "plt.xlabel('Question ID')\n",
    "plt.ylabel('Response time (seconds)')\n",
    "plt.title('Response Time for Questions with the Java tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_plot_df.plot(kind='bar',use_index=False);\n",
    "plt.xlabel('Question ID')\n",
    "plt.ylabel('Response time (seconds)')\n",
    "plt.title('Response Time for Questions with the Python tag')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "java_plot_df['response time'].to_frame().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_plot_df['response time'].to_frame().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. We believe that the response time is lower for questions related to Python (compare to Java). Contradict or confirm this assumption by estimating the proper statistic with bootstrapping. Visualize the 95% confidence intervals with box plots and describe your findings.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. Repeat the first analysis (D1) by using the proper statistic to measure the response time for the tags that appear at least 5000 times. Plot the distribution of the 10 tags with the fastest response time.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to time constraints we weren't able to finish this task :( "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task E: What's up with PySpark?\n",
    "__The number of questions asked regarding a specific topic reflect the publics interest on it. We are interested on the popularity of PySpark. Compute and plot the number of questions with the ```pyspark``` tag for 30-day time intervals. Do you notice any trend over time? Is there any correlation between time and number of questions?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Compute and plot the number of questions with the pyspark tag for 30-day time intervals.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we obtain a table with the questions (PostTypeId = 1) which have *pyspark* tag and with their creation date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|   Tags|        CreationDate|\n",
      "+-------+--------------------+\n",
      "|pyspark|2015-02-18T22:18:...|\n",
      "|pyspark|2015-02-19T09:10:...|\n",
      "|pyspark|2015-02-19T11:33:...|\n",
      "|pyspark|2015-02-20T00:01:...|\n",
      "+-------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "querye1=\"\"\"\n",
    "select Tags, CreationDate\n",
    "from postsb\n",
    "where PostTypeId = 1 and Tags = \"pyspark\"\n",
    "\"\"\"\n",
    "resulte1 = spark.sql(querye1)\n",
    "resulte1.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform this table to a pandas data frame in order to work with the date data and to plot the asked distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = resulte1.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform from string to datetime object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tags</th>\n",
       "      <th>CreationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pyspark</td>\n",
       "      <td>2015-02-18 22:18:59.567000+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pyspark</td>\n",
       "      <td>2015-02-19 09:10:54.780000+01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pyspark</td>\n",
       "      <td>2015-02-19 11:33:12.143000+01:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Tags                      CreationDate\n",
       "0  pyspark  2015-02-18 22:18:59.567000+01:00\n",
       "1  pyspark  2015-02-19 09:10:54.780000+01:00\n",
       "2  pyspark  2015-02-19 11:33:12.143000+01:00"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark[\"CreationDate\"] = df_pyspark[\"CreationDate\"].apply(lambda x: x[:23] + \"000\" + x[23:26] + x[27:])\n",
    "df_pyspark[\"CreationDate\"] = df_pyspark[\"CreationDate\"].apply(lambda x: datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%f%z\"))\n",
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform from datetime to timestamp and substract the timestamp of the first entry to the whole column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tags</th>\n",
       "      <th>CreationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pyspark</td>\n",
       "      <td>26005125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pyspark</td>\n",
       "      <td>26044240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pyspark</td>\n",
       "      <td>26052778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Tags  CreationDate\n",
       "0  pyspark      26005125\n",
       "1  pyspark      26044240\n",
       "2  pyspark      26052778"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark2 = df_pyspark.copy()\n",
    "df_pyspark2[\"CreationDate\"] = df_pyspark2[\"CreationDate\"].apply(lambda x: calendar.timegm(x.timetuple()))\n",
    "first_time = df_pyspark2[\"CreationDate\"].min()\n",
    "df_pyspark2[\"CreationDate\"] = df_pyspark2[\"CreationDate\"].apply(lambda x: x-first_time)\n",
    "df_pyspark2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we get the 30 days intervals dividing the CreationDate column by the number of seconds in 30 days and taking the floor, giving us an integer that will represent the interval to which that row belongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tags</th>\n",
       "      <th>CreationDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pyspark</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pyspark</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pyspark</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pyspark</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pyspark</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Tags  CreationDate\n",
       "0  pyspark            10\n",
       "1  pyspark            10\n",
       "2  pyspark            10\n",
       "3  pyspark            10\n",
       "4  pyspark            10"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seconds_per_30_days=3600*24*30\n",
    "df_pyspark2[\"CreationDate\"]=df_pyspark2[\"CreationDate\"].apply(lambda x: int(x/n_seconds_per_30_days))\n",
    "df_pyspark2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we sort the data frame by the interval number and plot a histogram with the number of questions per interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Questions with pyspark tag')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_pyspark2=df_pyspark2.sort_values(by=\"CreationDate\")\n",
    "#bins=number of intervals we have consider\n",
    "ax=df_pyspark2[\"CreationDate\"].hist(bins=df_pyspark2[\"CreationDate\"].max(), figsize=(10,4)) \n",
    "ax.set_xlabel(\"30 days INTERVALS\")\n",
    "ax.set_ylabel(\"Questions with pyspark tag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Do you notice any trend over time? Is there any correlation between time and number of questions?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the number of questions with the pyspark tag increases over time. This trend translates into a big correlation between both variables. To compute the correlation coefficient we first create a data frame with the data of both variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>30_days_interval</th>\n",
       "      <th>Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47</td>\n",
       "      <td>451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46</td>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   30_days_interval  Counts\n",
       "0                49     462\n",
       "1                47     451\n",
       "2                52     403\n",
       "3                41     386\n",
       "4                46     377"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_correlation=df_pyspark2[\"CreationDate\"].value_counts().to_frame()\n",
    "df_for_correlation=df_for_correlation.reset_index()\n",
    "df_for_correlation.columns=[\"30_days_interval\", \"Counts\"]\n",
    "df_for_correlation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>30_days_interval</th>\n",
       "      <th>Counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30_days_interval</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.880159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Counts</th>\n",
       "      <td>0.880159</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  30_days_interval    Counts\n",
       "30_days_interval          1.000000  0.880159\n",
       "Counts                    0.880159  1.000000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_correlation.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This correlation coefficient(0.880159) makes sense since it indicates a positive correlation between both variables (and as we have said before, one variable usually decreases as the other variable decreases and one variable usually increases while the other increases.). This confirms that pyspark is becoming more popular!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
