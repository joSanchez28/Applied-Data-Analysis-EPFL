{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    " <p><div class=\"lev1\"><a href=\"#Task-A.-Another-LEGO-brick-in-the-wall\"><span class=\"toc-item-num\">Task A.&nbsp;&nbsp;</span>Another LEGO brick in the wall</a></div>\n",
    " <p><div class=\"lev1\"><a href=\"#Task-B.-Drop-the-Bike\"><span class=\"toc-item-num\">Task B.&nbsp;&nbsp;</span>Drop the Bike</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "import zipfile\n",
    "import math\n",
    "from datetime import date, time, datetime\n",
    "pd.options.mode.chained_assignment = None\n",
    "# default='warn', Mutes warnings when copying a slice from a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A. Another LEGO brick in the wall\n",
    "\n",
    "**LEGO is a popular brand of toy building bricks. They are often sold in sets in order to build a specific object. Each set contains a number of parts in different shapes, sizes and colors. This database contains information on which parts are included in different LEGO sets. It was originally compiled to help people who owned some LEGO sets already figure out what other sets they could build with the pieces they had. **\n",
    "\n",
    "**This dataset contains the official LEGO colors, parts, inventories (i.e., sets of LEGO parts which assembled create an object in the LEGO world) and sets (i.e., sets of LEGO inventories which assembled create a LEGO ecosystem). The schema of the dataset can be shown in the following UML diagram: **\n",
    "\n",
    "![lego-schema](lego-schema.png)\n",
    "\n",
    "**In this task you have to apply the following Data Wrangling pipeline:**\n",
    "1. Load your data into `Pandas`\n",
    "* Explore it and clean its dirty parts\n",
    "* Use it to answer a set of queries\n",
    "\n",
    "**Each of these subtasks are described in detail below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1. Loading phase\n",
    "**Load all the csv files into different `DataFrames`. Use meaningful names for your `DataFrames` (e.g., the respective filenames).**\n",
    "\n",
    "** *Hint: You can load files without first unzipping them (for `Pandas` version >= 0.18.1).* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGO_DATA_FOLDER = DATA_FOLDER + '/lego'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we just load each file in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/lego/colors.csv.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4aa1d70d8c41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLEGO_DATA_FOLDER\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/colors.csv.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0minventories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLEGO_DATA_FOLDER\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/inventories.csv.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0minventory_parts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLEGO_DATA_FOLDER\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/inventory_parts.csv.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0minventory_sets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLEGO_DATA_FOLDER\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/inventory_sets.csv.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpart_categories\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLEGO_DATA_FOLDER\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/part_categories.csv.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, compression, allowZip64)\u001b[0m\n\u001b[0;32m   1111\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1113\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1114\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1115\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/lego/colors.csv.zip'"
     ]
    }
   ],
   "source": [
    "colors = pd.read_csv(LEGO_DATA_FOLDER + '/colors.csv.zip')\n",
    "inventories = pd.read_csv(LEGO_DATA_FOLDER + '/inventories.csv.zip')\n",
    "inventory_parts = pd.read_csv(LEGO_DATA_FOLDER + '/inventory_parts.csv.zip')\n",
    "inventory_sets = pd.read_csv(LEGO_DATA_FOLDER + '/inventory_sets.csv.zip')\n",
    "part_categories = pd.read_csv(LEGO_DATA_FOLDER + '/part_categories.csv.zip')\n",
    "parts = pd.read_csv(LEGO_DATA_FOLDER + '/parts.csv.zip')\n",
    "sets = pd.read_csv(LEGO_DATA_FOLDER + '/sets.csv.zip')\n",
    "themes = pd.read_csv(LEGO_DATA_FOLDER + '/themes.csv.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the different datasets that compose our database. As a best practice we make sure to set unique indices to all the dataframes. During the cleaning and querying we can use reset_index() if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = colors.set_index('id')\n",
    "inventories = inventories.set_index('id')\n",
    "\n",
    "# For this set combine inventory_id and set_id to a hierarchiacal index to make it unique\n",
    "inventory_sets_hier = inventory_sets.set_index(['inventory_id', 'set_id'])\n",
    "\n",
    "part_categories = part_categories.set_index('id')\n",
    "sets = sets.set_index('id')\n",
    "themes = themes.set_index('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not change the index for inventory parts since even combinig the 3 id's still did not give a unique index. Hence we leave it as such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method can be used to check for uniqueness of the indexes and check every different value and value format. This is useful when checking for inconsistencies. To check a particular dataframe, replace df with appropriate dataframe variable. This will be useful for the cleaning phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_uniqueness(df):\n",
    "\n",
    "    print('Is the index unique? ', df.index.is_unique, '\\n')\n",
    "    print('The index values are:')\n",
    "    print(df.index.unique().tolist())\n",
    "    print('\\nDo the columns have unique values?')\n",
    "    for c in df.columns:\n",
    "        printer = c.ljust(25) + str(df[c].is_unique)\n",
    "        print(printer)\n",
    "\n",
    "    print('\\nThe values of the columns are:')\n",
    "    for c in df.columns:\n",
    "        print(c, '      ', df[c].unique())\n",
    "        \n",
    "index_uniqueness(sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2. Cleaning phase\n",
    "**Explore the following columns from your dataset:**\n",
    "\n",
    "1. sets: year\n",
    "* inventory_parts: quantity\n",
    "\n",
    "**What is the time range of the sets? \n",
    "What is the average quantity of the inventory parts? \n",
    "Do you see any inconsistencies? \n",
    "Provide code that detects and cleans such inconsistencies and validates the coherence of your dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First. We define this method to inspect each dataframe for inconsistencies to aid us spot 'bad stuff' during the cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_report(df):\n",
    "\n",
    "    print(\"Types:\\n\")\n",
    "    print(df.dtypes, '\\n')\n",
    "    print('Is the index unique: ', df.index.is_unique, '\\n')\n",
    "    print('The index values are:\\n')\n",
    "    print(df.index.unique().tolist(), '\\n')\n",
    "    print('Do the columns have unique values?')\n",
    "    for c in df.columns:\n",
    "        printer = c.ljust(25) + str(df[c].is_unique)\n",
    "        print(printer)\n",
    "\n",
    "    print('\\n', 'What are these values:')\n",
    "    for c in df.columns:\n",
    "        print(c, '      ', df[c].unique())\n",
    "\n",
    "    print('\\n', 'First few entries:', '\\n', df.head(10))\n",
    "    print('\\n', 'Last few entries:', '\\n', df.tail(10))\n",
    "\n",
    "quick_report(themes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start cleaning the sets dataframe. We can see a few inconsistencies in the year field. We noticed the following:\n",
    "\n",
    "- some negative values. In that case we just eliminate the minus\n",
    "- 5 digit numbers. We deal with those by taking the first 4 digits of the number and ignoring the fifth\n",
    "- some years are expressed as 70s, 80s, etc. We decided to replace those with the first year of the decade, so 1970, etc\n",
    "\n",
    "We perform the appropiate substitutions and convert the column to a datetime object for future ease of manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change negative values to positive. Here the '-' are assumed to be typos and hence not dropped\n",
    "sets.year = sets['year'].apply(lambda x: x[1:] if x[0] == '-' else x)\n",
    "\n",
    "# make sure every year is a 4 digit number\n",
    "sets.year = sets['year'].apply(lambda x: x[:4])\n",
    "\n",
    "# remove 70s, 80s, ... and replace with 1970, 1980,...\n",
    "sets.year = sets['year'].apply(lambda x: '19' + x[:-1] if 's' in x else x)\n",
    "\n",
    "# convert year (string) to datetime objects\n",
    "sets.year = sets['year'].apply(lambda x: datetime.strptime(x,'%Y'))\n",
    "\n",
    "sets.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not done with sets. We notice that there are sets in this dataframe that contain 0 or -1 (?) parts. Upon further inspection we realized those are not actual lego sets but things like 'Star wars poster' or 'The LEGO Ideas Book'. Those are not useful for our purposes, we just delete them.\n",
    "\n",
    "As a sidenote, after this purge there are still sets that are not actual LEGO sets in this dataframe, we just can't decide on a threshold on the number of parts for what we could call a proper LEGO set. For example, in the query for the dominant color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retain only sets with 2 or more parts\n",
    "sets = sets[sets[\"num_parts\"] > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the inventory dataframe. Here we only do two changes\n",
    "\n",
    "- replace the -np.inf values with 0\n",
    "- change the quantities column to integer type. This is mostly for visual clarity and to avoid future mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace -np.inf with 0\n",
    "inventory_parts = inventory_parts.replace(-np.inf, 0)\n",
    "\n",
    "# change quantities to integers\n",
    "inventory_parts.quantity = inventory_parts.quantity.apply(lambda x: int(x))\n",
    "\n",
    "inventory_parts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is the colors dataframe. One simple change here\n",
    "\n",
    "- the 'unknown color' with id -1 gives us no information so we remove it\n",
    "\n",
    "As a note, we decided to leave the 'no color' value with id 9999 as is. This way if someone wanted to add more colors they have indices 1008 - 9998 free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'unknown' color with id = -1\n",
    "colors = colors[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the themes dataframe has som NaN values in the parent column (no parent). Since we don't like NaNs we set them to 0 and from now on we work under the assumption that having a parent_id of 0 means no parent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaNs with 0\n",
    "themes = themes.replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the datasets are fairly clean so we leave them as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3. Querying phase\n",
    "**Answer the following queries using the functionality of `Pandas`:**\n",
    "\n",
    "1. List the ids of the inventories that belong to sets that contain cars. (*Hint: Find a smart way to distinguish which sets contain cars based on the sets' name*).\n",
    "* Plot the distribution of part categories as a (horizontal) bar chart. Restrict yourself to the 20 largest part categories (in terms of the number of parts belonging to the category).\n",
    "* Find the dominant color of each set. Then, plot using a (horizontal) bar chart, the number of sets per dominant color. Color each bar with the respective color that it represents.\n",
    "* Create a scatter plot of the *luminance*\\* of the sets vs their publishing year. Do you see a trend over the last years? How do you interpret it?\n",
    "\n",
    "**The luminance of a color is a [measure of brightness](https://en.wikipedia.org/wiki/Luminance) which, given its RGB representation, can be computed as follows:**\n",
    "\n",
    "$luminance = \\sqrt{0.299*R^2 + 0.587*G^2 + 0.114*B^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1\n",
    "In order to distinguish which sets contain cars without going overboard (no semantics or external filters) we just choose set names that either:\n",
    "\n",
    "- Contain 'Car ' or 'Cars '. since those are capitalized we assume they are going to be the start of a word, we don't need to put a space before. But we do need the space after to avoid words that just start with 'Car' or 'Cars' like 'Carbonized' or 'Carse'\n",
    "- Contain ' car ' or ' cars '. That is 'car' or 'cars' as separate words in the title\n",
    "- Contain words that end with 'car' or 'cars'. This might seem counterintuitive, but a quick search on a web dictionary https://www.thefreedictionary.com/words-that-end-in-car shows us that the few words that end with car that are not actual cars are either very obscure like 'vicar' or very domain specific like 'trocar'. That means it is extremely unlikely that a bad word other than a proper name like 'Oscar' (again unlikely in a product name) will mess this filter. This is important beause we don't want to miss specific types of car like 'supercar' or 'motorcar'\n",
    "\n",
    "Then we put this data into a dataframe, merge it with inventory_sets to get the inventory_id's and label them properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set_ids for whom the name contains ' Cars ', ' cars ', ' Car ', ' car ' \n",
    "sets_car_cond = (sets.name.str.contains('Car ') | sets.name.str.contains('Cars ') | sets.name.str.contains(' car ') | sets.name.str.contains(' cars ') | sets.name.str.endswith('car') | sets.name.str.endswith('cars'))\n",
    "sets_car = sets[sets_car_cond]\n",
    "\n",
    "# merge this dataframe with inventory_sets, which will contain the inventory_id's\n",
    "# without any parameter, it will do an inner join (intersection between the 2 sets)\n",
    "sets_inventories_cars = pd.merge(sets_car,inventory_sets,left_index=True,right_on='set_id')[['set_id','name']]\n",
    "sets_inventories_cars\n",
    "\n",
    "# make the dataframe look cleaner with correct labels\n",
    "sets_inventories_cars = sets_inventories_cars.reset_index()\n",
    "sets_inventories_cars.columns = ['inventory_id','set_id','set_name']\n",
    "\n",
    "# print the resulting inventory_id's\n",
    "sets_inventories_cars = sorted(sets_inventories_cars['inventory_id'].unique())\n",
    "print('Here is the list of inventories:')\n",
    "print(sets_inventories_cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2\n",
    "We use pylab (imported at the beginning as pl) for the barchart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = parts.groupby(['part_cat_id'])['id'].count().sort_values(ascending=False).iloc[:20]\n",
    "new_res = res.to_frame('number of parts')\n",
    "new_res.plot(kind = 'barh',title='Plot showing the top 20 part_cat_id and and the number of parts')\n",
    "\n",
    "\n",
    "pl.xlabel('number of parts');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 3\n",
    "\n",
    "This querry can be divided into two querries:\n",
    "- 1) Find the most dominant color per sets\n",
    "- 2) For each dominant colors plot the number of sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1\n",
    "\n",
    "The technic here is to merge different data frames in order to obtain a dataframe with labels: set_id, inventory_id, color_id, quantity and color_name. Hence we will need to merge the following dataframes: sets, inventory_sets, inventory_parts and colors. We shall do it in the following order:\n",
    "- 1) Merge sets with inventory_sets with set_id as the common label\n",
    "- 2) Then merge the above resulting dataframe with inventory parts with inventory_id as the common label\n",
    "- 3) Finally merge the above resulting dataframe with colors with color_id as the common label \n",
    "\n",
    "Afterwards you need to take the resulting dataframe and group by set_id and color name to obtain the quantity for each colors of each sets. Then from the dataframe we can sort the values according to quantity (ascending = False) and just select the top color which will be the most dominant color for that particular set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first merge.\n",
    "inventory_sets_parts = pd.merge(sets,inventory_sets,left_index=True,right_on='set_id')[['set_id','inventory_id']]\n",
    "\n",
    "# The second merge\n",
    "inventory_sets_parts_color = pd.merge(inventory_sets_parts,inventory_parts)\n",
    "\n",
    "# Take appropriate columns (not all of them are useful).\n",
    "inventory_sets_parts_color = inventory_sets_parts_color[['set_id','color_id','quantity']]\n",
    "\n",
    "# Final merge.\n",
    "inventory_sets_parts_color_name = pd.merge(inventory_sets_parts_color, colors,right_index=True,left_on='color_id')[['set_id','name','quantity']]\n",
    "\n",
    "# Group by and sum in order to obtain the total quantities per colors per set.\n",
    "inventory_sets_parts_color_name = inventory_sets_parts_color_name.groupby(['set_id','name']).sum()\n",
    "\n",
    "# Sort the dataframe in order of decreasing quantities\n",
    "inventory_sets_parts_color_name = inventory_sets_parts_color_name.reset_index().sort_values(['set_id','quantity'], ascending = [True,False]).set_index(['set_id'])\n",
    "\n",
    "dominant_color = pd.DataFrame(columns=['set_id','color','quantity'])\n",
    "\n",
    "# For loop necessary since while iterating over each indexes of set_id (hierachical indexes), some sets have only 1 color\n",
    "# Which will be a single value and some sets will have multiple colors, which will be Series. So we created a new dataframe\n",
    "# and \n",
    "for i in inventory_sets_parts_color_name.index.unique():\n",
    "    the_colors_quantity = inventory_sets_parts_color_name.loc[i].quantity\n",
    "    the_colors_name = inventory_sets_parts_color_name.loc[i]\n",
    "    if isinstance(the_colors_quantity,pd.Series):\n",
    "        df_to_append = pd.DataFrame(data = [[i,the_colors_name.name.iloc[0],the_colors_quantity.iloc[0]]],columns=['set_id','color','quantity'])\n",
    "        dominant_color = pd.concat([dominant_color,df_to_append])\n",
    "    else:\n",
    "        df_to_append = pd.DataFrame(data = [[i,the_colors_name[0],the_colors_quantity]],columns=['set_id','color','quantity'])\n",
    "        dominant_color = pd.concat([dominant_color,df_to_append])\n",
    "\n",
    "\n",
    "    \n",
    "dominant_color_per_set_id = dominant_color.set_index('set_id')\n",
    "\n",
    "# Remove sets with dominant color quantity = 0, which means that the set has no dominant color.\n",
    "dominant_color_per_set_id = dominant_color_per_set_id.drop(dominant_color_per_set_id[dominant_color_per_set_id.quantity == 0.0].index)\n",
    "\n",
    "# Sort by quantities\n",
    "dominant_color_per_set_id = dominant_color_per_set_id.sort_values(by='quantity',ascending = False)\n",
    "\n",
    "dominant_color_per_set_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2\n",
    "\n",
    "For this querry, a lot of the work is already done in the previous part. So we can first get the list of dominant colors. Then the following merges:\n",
    "- 1) Merge with the dataframe from part 1 above\n",
    "- 2) From the merged dataframe, group by the color name and sum to obtain the number of sets per dominant colors\n",
    "- 3) Merge the above resulting dataset with colors in order to obtain the rgb value for each color.\n",
    "- 4) Plot the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary for x axis label\n",
    "import pylab as pl\n",
    "\n",
    "# Get the list of dominant colors (no duplicates) and convert to dataframe\n",
    "all_dominant_colors = dominant_color_per_set_id['color'].unique()\n",
    "all_dominant_colors = pd.DataFrame(all_dominant_colors,columns=['color name'])\n",
    "\n",
    "# First merge with group by and sum\n",
    "sets_per_colors = pd.merge(all_dominant_colors,inventory_sets_parts_color_name.reset_index(),left_on='color name',right_on='name')[['color name','set_id']].groupby('color name').count()\n",
    "\n",
    "# Rename column for consistency\n",
    "sets_per_colors.columns = ['number of sets']\n",
    "\n",
    "# Sort by decreasing number of sets\n",
    "sets_per_colors = sets_per_colors.sort_values(by='number of sets', ascending = False)\n",
    "sets_per_colors\n",
    "\n",
    "# Get the rgb value for each color, will be necessary for plot (In fact rgb code is hex code)\n",
    "sets_per_colors_rgb = pd.merge(sets_per_colors,colors,left_index=True, right_on='name')[['name','rgb','number of sets']]\n",
    "sets_per_colors_rgb.columns = ['color name', 'hex', 'number of sets']\n",
    "sets_per_colors_rgb = sets_per_colors_rgb.set_index('color name')\n",
    "\n",
    "# Give the rgb (hex format) the correct format\n",
    "sets_per_colors_rgb.hex = sets_per_colors_rgb.hex.apply(lambda x: '#'+x)\n",
    "\n",
    "# Plot graph\n",
    "plt.barh(sets_per_colors_rgb.index, sets_per_colors_rgb['number of sets'], color = sets_per_colors_rgb.hex.tolist(), edgecolor = 'black')\n",
    "pl.title('Plot showing the number of sets per dominant colors')\n",
    "pl.xlabel('number of sets')\n",
    "\n",
    "sets_per_colors_rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 4\n",
    "\n",
    "For this querry we re used the previous dataframe (most dominant colors per set, which is the color we used for each set to calculate the luminance) and just added the hex colors and years from colors and sets respectively. We then converted the hex color codes to rgb using the webcolors library (To install it: pip install webcolors==1.3) and then to the luinance using the formula. To plot the years, we converted the years back to integers only taking the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To conver hex to rgb\n",
    "import webcolors\n",
    "\n",
    "# First merge with colors to get the hex format (rgb label)\n",
    "df = pd.merge(dominant_color_per_set_id.reset_index(), colors, left_on='color', right_on='name')[['set_id','color','rgb']]\n",
    "\n",
    "# Second merge to get the publishing years\n",
    "df = pd.merge(df,sets,left_on='set_id', right_index=True)[['set_id','color','rgb','year']]\n",
    "\n",
    "# Convert hex formated colors to luminances via rgb format\n",
    "df.rgb = df.rgb.apply(lambda x: webcolors.hex_to_rgb('#'+x))\n",
    "df.rgb = df.rgb.apply(lambda x: np.sqrt(0.299*(x[0]^2) + 0.587*(x[1]^2) + 0.114*(x[2]^2)))\n",
    "\n",
    "# Rename columns for consistency.\n",
    "df.columns = ['set_id','color','luminance','year']\n",
    "\n",
    "df = df[['luminance','year']]\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Convert datetime years to int for plot.\n",
    "df.year = df.year.apply(lambda x: int(x.year))\n",
    "\n",
    "# Plot \n",
    "df.plot(kind='scatter',x='year',y='luminance', title = 'Luminance of sets vs their publishing year.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that as the years progress, luminance makes jumps. Over the last years, luminance was very low then very high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B. Drop the bike\n",
    "\n",
    "** *Los Angeles Metro* has been sharing publicly [anonymized *Metro Bike Share* trip data](https://bikeshare.metro.net/about/data/) under the [Open Database License (ODbL)](http://opendatacommons.org/licenses/odbl/1.0/).**\n",
    "\n",
    "**In this task you will again perform data wrangling and interpretation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1. Loading phase\n",
    "**Load the json file into a `DataFrame`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIKES_DATA_FOLDER = DATA_FOLDER + '/bikes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes = pd.read_json(BIKES_DATA_FOLDER + \"/metro-bike-share-trip-data.json.zip\")\n",
    "bikes.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2. Cleaning phase\n",
    "**Describe the type and the value range of each attribute. Indicate and transform the attributes that are `Categorical`. Are there redundant columns in the dataset (i.e., are there columns whose value depends only on the value of another column)? What are the possible pitfalls of having such columns? Reduce *data redundancy* by extracting such columns to separate `DataFrames`. Which of the two formats (the initial one or the one with reduced data redundancy) is more susceptible to inconsistencies? At the end print for each `Dataframe` the *type of each column* and it's *shape*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set the Trip ID field as index and check succesfully wether it is unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.set_index(\"Trip ID\")\n",
    "bikes.index.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from \"Start Time\" and \"End Time\", which we will deal with next, we find two explicitly categorical columns of data: \"Passholder Type\" and \"Trip Route Category\". We turn them into categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.Categorical(bikes[\"Trip Route Category\"]))\n",
    "print(pd.Categorical(bikes[\"Passholder Type\"]))\n",
    "bikes[\"Trip Route Category\"] = pd.Categorical(bikes[\"Trip Route Category\"])\n",
    "bikes[\"Passholder Type\"]     = pd.Categorical(bikes[\"Passholder Type\"])\n",
    "bikes.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform \"End Time\" and \"Start Time\" to datetime objects.\n",
    "\n",
    "We also notice that some values in the \"Duration\" column are unexpectedly large. Since this value is entirely dependent on the \"Start Time\" and \"End Time\" columns we recompute entirely to anticipate errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to datetime object\n",
    "bikes[\"End Time\"]   = bikes[\"End Time\"].apply(lambda x: datetime.strptime(x, '%Y-%m-%dT%H:%M:%S'))\n",
    "bikes[\"Start Time\"] = bikes[\"Start Time\"].apply(lambda x: datetime.strptime(x, '%Y-%m-%dT%H:%M:%S'))\n",
    "\n",
    "# compute the difference between end time and start time and transform to seconds\n",
    "bikes[\"Duration\"]   = pd.to_datetime(bikes[\"End Time\"]) - pd.to_datetime(bikes[\"Start Time\"])\n",
    "bikes[\"Duration\"]   = bikes[\"Duration\"].apply(lambda x: x.total_seconds())\n",
    "\n",
    "# convert to hours\n",
    "bikes[\"Duration\"] = bikes[\"Duration\"]/3600.0\n",
    "bikes.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice the following redundancies in our dataframe:\n",
    "\n",
    "- Duration could be redundant because we can get it from Start and End time data. We will be using this column though, so we leave it there.\n",
    "- With \"Trip Route Category\" we have a similar problem. Its value is \"Round Trip\" if Starting and Ending stations are the same and \"One Way\" otherwise, but since during the query phase we are going to be using this values we will leave this column as is for now.\n",
    "- The latitude and longitude of the stations is completely tied to the Station ID. We are not using this values so we create a sparate dataframe with Station ID -combined for Starting and Ending- as the (unique) index and \"Station Latitude\" and \"Station longitude\" as columns, then drop the latitude and longitude from the original table. Now we have a lookup table for the coordinates of each station and our original dataframe is more readable.\n",
    "- The same applies to \"Plan Duration\" being hardly tied to \"Passholder Type\". We also create a separate, smaller dataframe as a lookup table and leave \"Passholder Type\" only in the original frame, since we are going to be using this label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# starting stations\n",
    "df_starting_stations         = bikes[[\"Starting Station ID\", \"Starting Station Latitude\", \"Starting Station Longitude\"]]\n",
    "df_starting_stations.columns = [\"Station ID\", \"Station Latitude\", \"Station Longitude\"]\n",
    "\n",
    "# ending stations\n",
    "df_ending_stations           = bikes[[\"Ending Station ID\", \"Ending Station Latitude\", \"Ending Station Longitude\"]]\n",
    "df_ending_stations.columns   = [\"Station ID\", \"Station Latitude\", \"Station Longitude\"]\n",
    "\n",
    "# combine both and drop duplicates\n",
    "stations = pd.concat([df_starting_stations, df_ending_stations], axis=0)\n",
    "stations.drop_duplicates([\"Station ID\"], inplace=True)\n",
    "\n",
    "# index by Station ID\n",
    "stations.set_index(\"Station ID\")\n",
    "\n",
    "# delete rows with all NaN values, this is a lookup table\n",
    "stations.dropna(how=\"all\")\n",
    "\n",
    "stations.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_durations = pd.DataFrame([{'Passholder Type':\"Flex Pass\", \"Plan Duration\":365}, \n",
    "                               {'Passholder Type':\"Monthly Pass\", \"Plan Duration\":30}, \n",
    "                               {'Passholder Type':\"Staff Pass\", \"Plan Duration\":365}, \n",
    "                               {'Passholder Type':\"Walk-up\", \"Plan Duration\":0}])\n",
    "plan_durations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes = bikes[[\"Bike ID\", \"Duration\", \"End Time\", \"Ending Station ID\",\n",
    "               \"Passholder Type\", \"Start Time\", \"Starting Station ID\",\n",
    "               \"Trip ID\", \"Trip Route Category\"]]\n",
    "bikes.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original format seems to be more susceptible to inconsistencies that what we just created. If someone changes a value in a row, they need to be very mindful that other values might be dependent on it and change the rest accordingly. \n",
    "\n",
    "Ideally we would drop \"Duration\" and \"Trip Route Category\" and calculate on the go, otherwise inconsistencies in the table might lead to confusion down the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Range of the \"Start Time\" and \"End Time\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min Start Time\", bikes.sort_values(\"Start Time\")[\"Start Time\"].iloc[0])\n",
    "print(\"Max Start Time\", bikes.sort_values(\"Start Time\")[\"Start Time\"].iloc[-1])\n",
    "print(\"Min End Time\", bikes.sort_values(\"Start Time\")[\"End Time\"].iloc[0])\n",
    "print(\"Max End Time\", bikes.sort_values(\"Start Time\")[\"End Time\"].iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value range of the rest of noncategorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.describe().loc[[\"min\",\"max\"]]\n",
    "stations.describe().loc[[\"min\",\"max\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show the final types and sizes of the dataframes we will be working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Clean bikes frame types:\\n\", bikes.dtypes)\n",
    "print(\"\\nShape:\",bikes.shape[0])\n",
    "\n",
    "print('\\n', 'plan_durations\\n', plan_durations.dtypes)\n",
    "print(\"\\nShape:\",plan_durations.shape[0])\n",
    "\n",
    "print(\"\\ndf_station data frame\\n\", stations.dtypes)\n",
    "print(\"\\nShape:\",stations.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3. Querying phase\n",
    "Answer the following queries using the functionality of `Pandas`.\n",
    "\n",
    "1. Plot the *distribution* of the number of outgoing trips from each station in a histogram with 20 bins (Hint: each bin describes a range of counts, not stations).\n",
    "* Plot histograms for the *duration* and *trip starting hour in the day* attributes. For both the *duration*  and the *trip starting hour* use *discrete 1-hour intervals*. What do you observe in each plot? What are some popular values in the *duration* plot? Explain the local maxima and the trends you observe on the *trip starting hour* plot based on human behavior.\n",
    "* For each *trip route category*, calculate the proportion of trips by *passholder type* and present your results in *a stacked bar chart with normalized height*.\n",
    "* Considering only trips that begin in the morning hours (before noon), plot in *a single bar chart* the proportion of trips by *passholder type* and *trip route category*. Explain any outliers you observe.\n",
    "* Separate the hours of the day into two intervals that have (approximately) the same number of bikes leaving the stations. For each of the two intervals calculate the proportion of trips by *passholder type* and *trip route category*. Present your results in a `DataFrame` which has a unique, non-composite index. Does the proportion of trips depend on whether it is the first or second hour interval? Would the company have any significant benefit by creating a more complex paying scheme where monthly pass users would pay less in the first interval and (equally) more on the second one? Assume that the number of trips per interval will not change if the scheme changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3. Querying phase\n",
    "Answer the following queries using the functionality of `Pandas`.\n",
    "\n",
    "**1. Plot the *distribution* of the number of outgoing trips from each station in a histogram with 20 bins (Hint: each bin describes a range of counts, not stations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = bikes[\"Starting Station ID\"].value_counts().hist(bins=20)\n",
    "pl.suptitle(\"Sample Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.- Plot histograms for the *duration* and *trip starting hour in the day* attributes. For both the *duration*  and the *trip starting hour* use *discrete 1-hour intervals*. What do you observe in each plot? What are some popular values in the *duration* plot? Explain the local maxima and the trends you observe on the *trip starting hour* plot based on human behavior.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the presence of clear extreme outliers, even after recalculating the duration from the original times. This could be due to errors in the registration of the timestamp -that or in LA one can (and will) grab a bike for an entire week (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(bikes.Duration.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main observation is that an overwhelming majority of trips last less than one hour. Since in the full histogram it is quite to appreciate we also made one only with trips under 4 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.hist(bikes,column=\"Duration\", bins=range(0,137))\n",
    "pd.DataFrame.hist(bikes,column=\"Duration\", bins=range(0,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second plot we first discretize the data on starting times.\n",
    "\n",
    "Firstly, we notice that the time interval from 7 to 20 has clearly more departues. This seems is clearly due to daytime.\n",
    "\n",
    "Secondly, we observe the maximal activity at around 17. One reasonable explanation seems that a lot of people finish their workday at this time, but we cannot make such inference with much confidence anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretize the values\n",
    "tmp = bikes[\"Start Time\"].apply(lambda x: x.hour)\n",
    "\n",
    "pd.DataFrame.hist(tmp.to_frame(),column=\"Start Time\", bins=range(0,25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.- For each *trip route category*, calculate the proportion of trips by *passholder type* and present your results in *a stacked bar chart with normalized height*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cold hard calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = bikes[bikes[\"Trip Route Category\"]==\"One Way\"]\n",
    "\n",
    "total_One_Way_trips = len(tmp)\n",
    "staff_trips         = len(tmp[tmp[\"Passholder Type\"]==\"Staff Annual\"])\n",
    "monthly_pass_trips  = len(tmp[tmp[\"Passholder Type\"]==\"Monthly Pass\"])\n",
    "flex_pass_trips     = len(tmp[tmp[\"Passholder Type\"]==\"Flex Pass\"])\n",
    "walk_up_trips       = len(tmp[tmp[\"Passholder Type\"]==\"Walk-up\"])\n",
    "\n",
    "prop_staff   = staff_trips / total_One_Way_trips\n",
    "prop_monthly = monthly_pass_trips / total_One_Way_trips\n",
    "prop_flex    = flex_pass_trips / total_One_Way_trips\n",
    "prop_walk_up = walk_up_trips / total_One_Way_trips\n",
    "\n",
    "print(\"For \\\"One Way\\\" category:\")\n",
    "print(\"Prop. of \\\"Staff Annual\\\" =\",prop_staff)\n",
    "print(\"Prop. of \\\"Monthly pass\\\" =\", prop_monthly)\n",
    "print(\"Prop. of \\\"Flex Pass\\\" =\",prop_flex)\n",
    "print(\"Prop. \\\"Walk-up\\\" =\",prop_walk_up)\n",
    "# must add up to 1\n",
    "print(\"Sum = \",prop_staff + prop_monthly + prop_flex + prop_walk_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = (bikes[bikes[\"Trip Route Category\"]==\"Round Trip\"])\n",
    "\n",
    "total_Round_Trip_trips = len(tmp)\n",
    "staff_trips            = len(tmp[tmp[\"Passholder Type\"]==\"Staff Annual\"])\n",
    "monthly_pass_trips     = len(tmp[tmp[\"Passholder Type\"]==\"Monthly Pass\"])\n",
    "flex_pass_trips        = len(tmp[tmp[\"Passholder Type\"]==\"Flex Pass\"])\n",
    "walk_up_trips          = len(tmp[tmp[\"Passholder Type\"]==\"Walk-up\"])\n",
    "\n",
    "prop_staff2   = staff_trips / total_Round_Trip_trips\n",
    "prop_monthly2 = monthly_pass_trips / total_Round_Trip_trips\n",
    "prop_flex2    = flex_pass_trips / total_Round_Trip_trips\n",
    "prop_walk_up2 = walk_up_trips / total_Round_Trip_trips\n",
    "\n",
    "print(\"For \\\"Round Trip\\\" category:\")\n",
    "print(\"Prop. of \\\"Staff Annual\\\" =\",prop_staff2)\n",
    "print(\"Prop. of \\\"Monthly pass\\\" =\", prop_monthly2)\n",
    "print(\"Prop. of \\\"Flex Pass\\\" =\",prop_flex2)\n",
    "print(\"Prop. \\\"Walk-up\\\" =\",prop_walk_up2)\n",
    "#must add up to 1\n",
    "print(\"Sum = \",prop_staff2 + prop_monthly2 + prop_flex2 + prop_walk_up2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = np.array([[prop_staff ,  prop_monthly,  prop_flex, prop_walk_up],\n",
    "                    [prop_staff2,  prop_monthly2,  prop_flex2, prop_walk_up2]])\n",
    "\n",
    "pd.DataFrame(my_data, index=[\"One Way\",\"Round Trip\"]).plot(kind='bar', stacked=True)\n",
    "pl.suptitle(\"Passholder Type by Trip Route\")\n",
    "pl.legend([\"Staff\", \"Monthly\", \"Flex\", \"Walk-up\"])\n",
    "pl.xlabel(\"Route Trip Category\")\n",
    "pl.ylabel(\"Proportion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.- Considering only trips that begin in the morning hours (before noon), plot in *a single bar chart* the proportion of trips by *passholder type* and *trip route category*. Explain any outliers you observe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we consider hours from 5 to 12 (the morning hours).\n",
    "df_tmp = bikes[bikes[\"Start Time\"].apply(lambda x: x.hour < 12 and x.hour>5)]\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trips = len(df_tmp)\n",
    "prop_staff = 100*len(df_tmp[df_tmp[\"Passholder Type\"]==\"Staff Annual\"])/total_trips\n",
    "prop_monthly = 100*len(df_tmp[df_tmp[\"Passholder Type\"]==\"Monthly Pass\"])/total_trips\n",
    "prop_flex = 100*len(df_tmp[df_tmp[\"Passholder Type\"]==\"Flex Pass\"])/total_trips\n",
    "prop_walk_up= 100*len(df_tmp[df_tmp[\"Passholder Type\"]==\"Walk-up\"])/total_trips\n",
    "\n",
    "prop_one_way=100*len(df_tmp[df_tmp[\"Trip Route Category\"]==\"One Way\"])/total_trips\n",
    "prop_round_trip=100*len(df_tmp[df_tmp[\"Trip Route Category\"]==\"Round Trip\"])/total_trips\n",
    "\n",
    "my_data = np.array([[prop_staff ,  prop_monthly,  prop_flex, prop_walk_up], [prop_one_way,  prop_round_trip,0,0]])\n",
    "pd.DataFrame(my_data, index=[\"Passholder Type\", \"Trip route category\"]).plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe in the **first chart** that during these hours the majority of the travellers have a \"Monthly Pass\"(orange portion) and also that there are so few travellers with the \"Staff Annual\"(blue portion). <br>In the **second chart** we see that the overwhelming majority of the travellers do a \"One Way\" trip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.- Separate the hours of the day into two intervals that have (approximately) the same number of bikes leaving the stations. For each of the two intervals calculate the proportion of trips by *passholder type* and *trip route category*. Present your results in a `DataFrame` which has a unique, non-composite index. Does the proportion of trips depend on whether it is the first or second hour interval? Would the company have any significant benefit by creating a more complex paying scheme where monthly pass users would pay less in the first interval and (equally) more on the second one? Assume that the number of trips per interval will not change if the scheme changes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we have to find the two hours intervals that have(approximately) the same number of departures.\n",
    "For doing that, we create a temporal data frame and change the \"Start Time\" column for getting just the hours data. Then, we sort the data frame by the \"Start Time\" and finally we divide the data frame in two parts. \n",
    "\n",
    "We see that the last trip have the \"Starting Time\" 14, so we decide to take the intervals: Period1 = [0,14] and Period2 = [15,23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp=bikes.copy()\n",
    "df_tmp[\"Start Time\"]=df_tmp[\"Start Time\"].apply(lambda x: x.hour)\n",
    "\n",
    "df_tmp.sort_values(ascending=True, by=\"Start Time\", inplace=True)\n",
    "df_tmp[:math.floor(len(df_tmp)/2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we calculate the proportion of trips by *passholder type* and *trip route category* for each of the two intervals and present our results in a `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we observed that slicing the day in two intervals Period1 = [0,14] and Period2 = [15,23] \n",
    "#we get around the same number of bikes leaving the stations in each period\n",
    "df_p1 = df_tmp[df_tmp[\"Start Time\"].apply(lambda d: d < 15)]\n",
    "df_p2 = df_tmp[df_tmp[\"Start Time\"].apply(lambda d: d >= 15)]\n",
    "\n",
    "#Proportions computation:\n",
    "total_trips1  = len(df_p1)\n",
    "prop_staff1   = len(df_p1[df_p1[\"Passholder Type\"]==\"Staff Annual\"])/total_trips1\n",
    "prop_monthly1 = len(df_p1[df_p1[\"Passholder Type\"]==\"Monthly Pass\"])/total_trips1\n",
    "prop_flex1    = len(df_p1[df_p1[\"Passholder Type\"]==\"Flex Pass\"])/total_trips1\n",
    "prop_walk_up1 = len(df_p1[df_p1[\"Passholder Type\"]==\"Walk-up\"])/total_trips1\n",
    "\n",
    "prop_one_way1    = len(df_p1[df_p1[\"Trip Route Category\"]==\"One Way\"])/total_trips1\n",
    "prop_round_trip1 = len(df_p1[df_p1[\"Trip Route Category\"]==\"Round Trip\"])/total_trips1\n",
    "\n",
    "total_trips2  = len(df_p2)\n",
    "prop_staff2   = len(df_p2[df_p2[\"Passholder Type\"]==\"Staff Annual\"])/total_trips2\n",
    "prop_monthly2 = len(df_p2[df_p2[\"Passholder Type\"]==\"Monthly Pass\"])/total_trips2\n",
    "prop_flex2    = len(df_p2[df_p2[\"Passholder Type\"]==\"Flex Pass\"])/total_trips2\n",
    "prop_walk_up2 = len(df_p2[df_p2[\"Passholder Type\"]==\"Walk-up\"])/total_trips2\n",
    "\n",
    "prop_one_way2=len(df_p2[df_p2[\"Trip Route Category\"]==\"One Way\"])/total_trips2\n",
    "prop_round_trip2=len(df_p2[df_p2[\"Trip Route Category\"]==\"Round Trip\"])/total_trips2\n",
    "\n",
    "df_prop1=pd.DataFrame([{'Period':1, 'Passholder Type':\"Flex Pass\",\"PT Market Share\":prop_flex1 },\n",
    "                    {'Period':1,'Passholder Type':\"Monthly Pass\", \"PT Market Share\":prop_monthly1 },\n",
    "                    {'Period':1,'Passholder Type':\"Staff Pass\", \"PT Market Share\":prop_staff1},\n",
    "                    {'Period':1,'Passholder Type':\"Walk-up\", \"PT Market Share\":prop_walk_up1},\n",
    "                    {'Period':2, 'Passholder Type':\"Flex Pass\",\"PT Market Share\":prop_flex2 },\n",
    "                    {'Period':2,'Passholder Type':\"Monthly Pass\", \"PT Market Share\":prop_monthly2 },\n",
    "                    {'Period':2,'Passholder Type':\"Staff Pass\", \"PT Market Share\":prop_staff2},\n",
    "                    {'Period':2,'Passholder Type':\"Walk-up\", \"PT Market Share\":prop_walk_up2}])\n",
    "\n",
    "df_prop2=pd.DataFrame([{'Period':1, 'Trip Route Category':\"One Way\",\"TRC Market Share\":prop_one_way1 },\n",
    "                    {'Period':1,'Trip Route Category':\"Round Trip\", \"TRC Market Share\":prop_round_trip1 },\n",
    "                    {'Period':2, 'Trip Route Category':\"One Way\",\"TRC Market Share\":prop_one_way2 },\n",
    "                    {'Period':2,'Trip Route Category':\"Round Trip\", \"TRC Market Share\":prop_round_trip2 },])\n",
    "\n",
    "df_prop=pd.merge(df_prop1,df_prop2) #We are asked to give a DataFrame\n",
    "\n",
    "df_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does the proportion of trips depend on whether it is the first or second hour interval? **\n",
    "<br>The proportion is always similar (independently of the period)\n",
    "\n",
    "\n",
    "**Would the company have any significant benefit by creating a more complex paying scheme where monthly pass users would pay less in the first interval and (equally) more on the second one? Assume that the number of trips per interval will not change if the scheme changes.**\n",
    "<br>If monthly pass users pay less in the first interval, the company would earn less money because the market share in the first period is greater(for this kind of users)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
